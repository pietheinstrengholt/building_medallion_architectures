{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb010e8-ab99-47e4-b459-6c07a625c215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3511281742453697>, line 13\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Fetch schema data from landing zone\u001B[39;00m\n",
       "\u001B[1;32m     11\u001B[0m jsonSchema \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/buildingma/volumes/mnt_landing/\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;132;01m{\u001B[39;00mschemaName\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilePath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtableName\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mjson()\n",
       "\u001B[0;32m---> 13\u001B[0m ddl \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mtypes\u001B[38;5;241m.\u001B[39mDataType \\\n",
       "\u001B[1;32m     14\u001B[0m   \u001B[38;5;241m.\u001B[39mfromJson(jsonSchema)\u001B[38;5;241m.\u001B[39mtoDDL()\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Migrate parquet data to delta files\u001B[39;00m\n",
       "\u001B[1;32m     17\u001B[0m (spark\u001B[38;5;241m.\u001B[39mreadStream\n",
       "\u001B[1;32m     18\u001B[0m   \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcloudFiles\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     19\u001B[0m   \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcloudFiles.format\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     32\u001B[0m   \u001B[38;5;241m.\u001B[39mtoTable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbronze_adventureworks.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtableName\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     33\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:967\u001B[0m, in \u001B[0;36mSparkSession.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m    965\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jconf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jvm\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsparkSession\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparkContext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnewSession\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\u001B[0;32m--> 967\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n",
       "\u001B[1;32m    968\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVM_ATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n",
       "\u001B[1;32m    969\u001B[0m         )\n",
       "\u001B[1;32m    970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name)\n",
       "\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m: [JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'sparkContext' is not supported on shared clusters. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkAttributeError",
        "evalue": "[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'sparkContext' is not supported on shared clusters. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog"
       },
       "metadata": {
        "errorSummary": "[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'sparkContext' is not supported on shared clusters. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "JVM_ATTRIBUTE_NOT_SUPPORTED",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-3511281742453697>, line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Fetch schema data from landing zone\u001B[39;00m\n\u001B[1;32m     11\u001B[0m jsonSchema \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/buildingma/volumes/mnt_landing/\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;132;01m{\u001B[39;00mschemaName\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilePath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtableName\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mjson()\n\u001B[0;32m---> 13\u001B[0m ddl \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mtypes\u001B[38;5;241m.\u001B[39mDataType \\\n\u001B[1;32m     14\u001B[0m   \u001B[38;5;241m.\u001B[39mfromJson(jsonSchema)\u001B[38;5;241m.\u001B[39mtoDDL()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Migrate parquet data to delta files\u001B[39;00m\n\u001B[1;32m     17\u001B[0m (spark\u001B[38;5;241m.\u001B[39mreadStream\n\u001B[1;32m     18\u001B[0m   \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcloudFiles\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m   \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcloudFiles.format\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     32\u001B[0m   \u001B[38;5;241m.\u001B[39mtoTable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbronze_adventureworks.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtableName\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:967\u001B[0m, in \u001B[0;36mSparkSession.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jconf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jvm\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsparkSession\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparkContext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnewSession\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 967\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n\u001B[1;32m    968\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVM_ATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n\u001B[1;32m    969\u001B[0m         )\n\u001B[1;32m    970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name)\n",
        "\u001B[0;31mPySparkAttributeError\u001B[0m: [JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'sparkContext' is not supported on shared clusters. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch parameters from Azure Data Factory\n",
    "#schemaName=dbutils.widgets.get(\"schemaName\")\n",
    "#tableName=dbutils.widgets.get(\"tableName\")\n",
    "#filePath=dbutils.widgets.get(\"filePath\")\n",
    "\n",
    "schemaName=\"adventureworks\"\n",
    "tableName=\"Address\"\n",
    "filePath=\"20241022\"\n",
    "\n",
    "# Fetch schema data from landing zone\n",
    "jsonSchema = spark.read.parquet(f\"/Volumes/buildingma/volumes/mnt_landing/\\\n",
    "{schemaName}/{filePath}/{tableName}.parquet\").schema.json()\n",
    "ddl = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType \\\n",
    "  .fromJson(jsonSchema).toDDL()\n",
    "\n",
    "# Migrate parquet data to delta files\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\n",
    "  .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "  .option(\"cloudFiles.backfillInterval\", \"1 day\")\n",
    "  .option(\"cloudFiles.schemaLocation\", f\"/Volumes/buildingma/volumes/mnt_landing/\\\n",
    "  {schemaName}/_checkpoint/{tableName}_autoload/\")\n",
    "  .schema(ddl)\n",
    "  .load(f\"/Volumes/buildingma/volumes/mnt_landing/\\\n",
    "  {schemaName}/{filePath}/{tableName}.parquet\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", f\"/Volumes/buildingma/volumes/mnt_landing/\\\n",
    "  {schemaName}/_checkpoint/{tableName}_autoload/\")\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(f\"bronze_adventureworks.{tableName}\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AutoLoader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}