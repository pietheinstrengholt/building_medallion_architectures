{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54fcc271-889c-4d2a-b833-ea77945b0202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the input widgets\n",
    "schemaName = dbutils.widgets.get(\"schemaName\")\n",
    "tableName = dbutils.widgets.get(\"tableName\")\n",
    "primaryKey = dbutils.widgets.get(\"primaryKey\")\n",
    "snapshotDate = dbutils.widgets.get(\"snapshotDate\")\n",
    "\n",
    "# Import functions\n",
    "from pyspark import *\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "\n",
    "# Fetch data from Bronze or intermediate Silver layer\n",
    "df = spark.read.table(f\"{schemaName}.{tableName}\") \\\n",
    ".filter(f\"snapshot_date = '{snapshotDate}'\")  \n",
    "\n",
    "# Remove loading_date column from dataset\n",
    "dataChanged = dataChanged.drop('loading_date')\n",
    "\n",
    "# Create var with all column names\n",
    "columnNames = dataChanged.schema.names\n",
    "\n",
    "# Generate hash key if primary is missing\n",
    "if not primaryKey or primaryKey == \"\":\n",
    "    dataChanged = dataChanged.withColumn(\"hash\", \\\n",
    "    sha2(concat_ws(\"||\", *dataChanged.columns), 256))\n",
    "    primaryKey = 'hash'\n",
    "\n",
    "# Create list with all columns\n",
    "columnNames = dataChanged.schema.names\n",
    "\n",
    "# Set date\n",
    "import datetime\n",
    "current_date = datetime.date.today()\n",
    "\n",
    "# Try and read existing dataset\n",
    "try:\n",
    "    # Read original data - this is your scd type 2 table holding all data\n",
    "    dataOriginal = spark.sql(f\"SELECT * FROM silver.hist_{tableName}\")\n",
    "except:\n",
    "    # Use first load when no data exists yet\n",
    "    newOriginalData = dataChanged.withColumn('current', lit(True)) \\\n",
    "    .withColumn('effectiveDate', lit(current_date)) \\\n",
    "    .withColumn('endDate', lit(datetime.date(9999, 12, 31)))\n",
    "    newOriginalData.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"silver.hist_{tableName}\")\n",
    "    newOriginalData.show()\n",
    "    newOriginalData.printSchema()\n",
    "    dbutils.notebook.exit(\"Done loading data! Newly loaded data \\\n",
    "    will be used to generate original data.\")\n",
    "\n",
    "# Prepare for merge, rename columns of newly loaded data, append 'src_'\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Rename all columns in dataChanged, prepend src_, and add additional columns\n",
    "df_new = dataChanged.select([F.col(c).alias(\"src_\"+c) \\\n",
    "for c in dataChanged.columns])\n",
    "src_columnNames = df_new.schema.names\n",
    "df_new2 = df_new.withColumn('src_current', lit(True)) \\\n",
    ".withColumn('src_effectiveDate', lit(current_date)) \\\n",
    ".withColumn('src_endDate', lit(datetime.date(9999, 12, 31)))\n",
    "\n",
    "import hashlib\n",
    "\n",
    "# Create dynamic columns\n",
    "src_primaryKey = 'src_' + primaryKey\n",
    "\n",
    "# FULL Merge, join on key column and also \n",
    "# date column to make only join to the latest records\n",
    "df_merge = dataOriginal.join(df_new2, (df_new2[src_primaryKey] \\\n",
    "== dataOriginal[primaryKey]), how='fullouter')\n",
    "\n",
    "# Derive new column to indicate the action\n",
    "df_merge = df_merge.withColumn('action',\n",
    "    when(md5(concat_ws('+', *columnNames)) == \\\n",
    "    md5(concat_ws('+', *src_columnNames)), 'NOACTION')\n",
    "    .when(df_merge.current == False, 'NOACTION')\n",
    "    .when(df_merge[src_primaryKey].isNull() & df_merge.current, 'DELETE')\n",
    "    .when(df_merge[src_primaryKey].isNull(), 'INSERT')\n",
    "    .otherwise('UPDATE')\n",
    ")\n",
    "\n",
    "# Generate target selections based on action codes\n",
    "column_names = columnNames + ['current', 'effectiveDate', 'endDate']\n",
    "src_column_names = src_columnNames + ['src_current', \\\n",
    "'src_effectiveDate', 'src_endDate']\n",
    "\n",
    "# Generate target selections based on action codes\n",
    "column_names = columnNames + ['current', 'effectiveDate', 'endDate']\n",
    "src_column_names = src_columnNames + ['src_current', \\\n",
    "'src_effectiveDate', 'src_endDate']\n",
    "\n",
    "# For records that needs no action\n",
    "df_merge_p1 = df_merge.filter(df_merge.action == \\\n",
    "'NOACTION').select(column_names)\n",
    "\n",
    "# For records that needs insert only\n",
    "df_merge_p2 = df_merge.filter(df_merge.action == \\\n",
    "'INSERT').select(src_column_names)\n",
    "df_merge_p2_1 = df_merge_p2.select([F.col(c) \\\n",
    ".alias(c.replace(c[0:4], \"\")) for c in df_merge_p2.columns])\n",
    "\n",
    "# For records that needs to be deleted\n",
    "df_merge_p3 = df_merge.filter(df_merge.action == \\\n",
    "'DELETE').select(column_names).withColumn('current', lit(False)) \\\n",
    ".withColumn('endDate', lit(current_date))\n",
    "\n",
    "# For records that needs to be expired and then inserted\n",
    "df_merge_p4_1 = df_merge.filter(df_merge.action == \\\n",
    "'UPDATE').select(src_column_names)\n",
    "df_merge_p4_2 = df_merge_p4_1.select([F.col(c) \\\n",
    ".alias(c.replace(c[0:4], \"\")) for c in df_merge_p2.columns])\n",
    "\n",
    "# Replace src_ alias in all columns\n",
    "df_merge_p4_3 = df_merge.filter(df_merge.action == \\\n",
    "'UPDATE').withColumn('endDate', date_sub(df_merge.src_effectiveDate, 1)) \\\n",
    ".withColumn('current', lit(False)).select(column_names)\n",
    "\n",
    "# Union all records together\n",
    "df_merge_final = df_merge_p1.unionAll(df_merge_p2) \\\n",
    ".unionAll(df_merge_p3).unionAll(df_merge_p4_2).unionAll(df_merge_p4_3)\n",
    "\n",
    "# At last, you can overwrite existing data using this new data frame\n",
    "df_merge_final.write.format(\"delta\").mode(\"overwrite\") \\\n",
    ".saveAsTable(schemaName + \".hist_\" + tableName)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Slowly_changing_dimension",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
